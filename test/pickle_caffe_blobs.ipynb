{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tools/pickle_caffe_blobs.py\n",
    "#!/usr/bin/env python2\n",
    "\n",
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "##############################################################################\n",
    "\n",
    "\"\"\"Script for converting Caffe (<= 1.0) models into the the simple state dict\n",
    "format used by Detectron. For example, this script can convert the orignal\n",
    "ResNet models released by MSRA.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from caffe.proto import caffe_pb2\n",
    "from caffe2.proto import caffe2_pb2\n",
    "from caffe2.python import caffe_translator\n",
    "from caffe2.python import utils\n",
    "from google.protobuf import text_format\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Dump weights from a Caffe model'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--prototxt',\n",
    "        dest='prototxt_file_name',\n",
    "        help='Network definition prototxt file path',\n",
    "        default=None,\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--caffemodel',\n",
    "        dest='caffemodel_file_name',\n",
    "        help='Pretrained network weights file path',\n",
    "        default=None,\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='out_file_name',\n",
    "        help='Output file path',\n",
    "        default=None,\n",
    "        type=str\n",
    "    )\n",
    "\n",
    "    if len(sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def normalize_resnet_name(name):\n",
    "    if name.find('res') == 0 and name.find('res_') == -1:\n",
    "        # E.g.,\n",
    "        #  res4b11_branch2c -> res4_11_branch2c\n",
    "        #  res2a_branch1 -> res2_0_branch1\n",
    "        chunk = name[len('res'):name.find('_')]\n",
    "        name = (\n",
    "            'res' + chunk[0] + '_' + str(\n",
    "                int(chunk[2:]) if len(chunk) > 2  # e.g., \"b1\" -> 1\n",
    "                else ord(chunk[1]) - ord('a')\n",
    "            ) +  # e.g., \"a\" -> 0\n",
    "            name[name.find('_'):]\n",
    "        )\n",
    "    return name\n",
    "\n",
    "\n",
    "def pickle_weights(out_file_name, weights):\n",
    "    blobs = {\n",
    "        normalize_resnet_name(blob.name): utils.Caffe2TensorToNumpyArray(blob)\n",
    "        for blob in weights.protos\n",
    "    }\n",
    "    with open(out_file_name, 'w') as f:\n",
    "        pickle.dump(blobs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('Wrote blobs:')\n",
    "    print(sorted(blobs.keys()))\n",
    "\n",
    "\n",
    "def add_missing_biases(caffenet_weights):\n",
    "    for layer in caffenet_weights.layer:\n",
    "        if layer.type == 'Convolution' and len(layer.blobs) == 1:\n",
    "            num_filters = layer.blobs[0].shape.dim[0]\n",
    "            bias_blob = caffe_pb2.BlobProto()\n",
    "            bias_blob.data.extend(np.zeros(num_filters))\n",
    "            bias_blob.num, bias_blob.channels, bias_blob.height = 1, 1, 1\n",
    "            bias_blob.width = num_filters\n",
    "            layer.blobs.extend([bias_blob])\n",
    "\n",
    "\n",
    "def remove_spatial_bn_layers(caffenet, caffenet_weights):\n",
    "    # Layer types associated with spatial batch norm\n",
    "    remove_types = ['BatchNorm', 'Scale']\n",
    "\n",
    "    def _remove_layers(net):\n",
    "        for i in reversed(range(len(net.layer))):\n",
    "            if net.layer[i].type in remove_types:\n",
    "                net.layer.pop(i)\n",
    "\n",
    "    # First remove layers from caffenet proto\n",
    "    _remove_layers(caffenet)\n",
    "    # We'll return these so we can save the batch norm parameters\n",
    "    bn_layers = [\n",
    "        layer for layer in caffenet_weights.layer if layer.type in remove_types\n",
    "    ]\n",
    "    _remove_layers(caffenet_weights)\n",
    "\n",
    "    def _create_tensor(arr, shape, name):\n",
    "        t = caffe2_pb2.TensorProto()\n",
    "        t.name = name\n",
    "        t.data_type = caffe2_pb2.TensorProto.FLOAT\n",
    "        t.dims.extend(shape.dim)\n",
    "        t.float_data.extend(arr)\n",
    "        assert len(t.float_data) == np.prod(t.dims), 'Data size, shape mismatch'\n",
    "        return t\n",
    "\n",
    "    bn_tensors = []\n",
    "    for (bn, scl) in zip(bn_layers[0::2], bn_layers[1::2]):\n",
    "        assert bn.name[len('bn'):] == scl.name[len('scale'):], 'Pair mismatch'\n",
    "        blob_out = 'res' + bn.name[len('bn'):] + '_bn'\n",
    "        bn_mean = np.asarray(bn.blobs[0].data)\n",
    "        bn_var = np.asarray(bn.blobs[1].data)\n",
    "        scale = np.asarray(scl.blobs[0].data)\n",
    "        bias = np.asarray(scl.blobs[1].data)\n",
    "        std = np.sqrt(bn_var + 1e-5)\n",
    "        new_scale = scale / std\n",
    "        new_bias = bias - bn_mean * scale / std\n",
    "        new_scale_tensor = _create_tensor(\n",
    "            new_scale, bn.blobs[0].shape, blob_out + '_s'\n",
    "        )\n",
    "        new_bias_tensor = _create_tensor(\n",
    "            new_bias, bn.blobs[0].shape, blob_out + '_b'\n",
    "        )\n",
    "        bn_tensors.extend([new_scale_tensor, new_bias_tensor])\n",
    "    return bn_tensors\n",
    "\n",
    "\n",
    "def remove_layers_without_parameters(caffenet, caffenet_weights):\n",
    "    for i in reversed(range(len(caffenet_weights.layer))):\n",
    "        if len(caffenet_weights.layer[i].blobs) == 0:\n",
    "            # Search for the corresponding layer in caffenet and remove it\n",
    "            name = caffenet_weights.layer[i].name\n",
    "            found = False\n",
    "            for j in range(len(caffenet.layer)):\n",
    "                if caffenet.layer[j].name == name:\n",
    "                    caffenet.layer.pop(j)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found and name[-len('_split'):] != '_split':\n",
    "                print('Warning: layer {} not found in caffenet'.format(name))\n",
    "            caffenet_weights.layer.pop(i)\n",
    "\n",
    "\n",
    "def normalize_shape(caffenet_weights):\n",
    "    for layer in caffenet_weights.layer:\n",
    "        for blob in layer.blobs:\n",
    "            shape = (blob.num, blob.channels, blob.height, blob.width)\n",
    "            if len(blob.data) != np.prod(shape):\n",
    "                shape = tuple(blob.shape.dim)\n",
    "                if len(shape) == 1:\n",
    "                    # Handle biases\n",
    "                    shape = (1, 1, 1, shape[0])\n",
    "                if len(shape) == 2:\n",
    "                    # Handle InnerProduct layers\n",
    "                    shape = (1, 1, shape[0], shape[1])\n",
    "                assert len(shape) == 4\n",
    "                blob.num, blob.channels, blob.height, blob.width = shape\n",
    "\n",
    "\n",
    "def load_and_convert_caffe_model(prototxt_file_name, caffemodel_file_name):\n",
    "    ipdb.set_trace()\n",
    "    caffenet = caffe_pb2.NetParameter()\n",
    "    caffenet_weights = caffe_pb2.NetParameter()\n",
    "    text_format.Merge(open(prototxt_file_name).read(), caffenet)\n",
    "    caffenet_weights.ParseFromString(open(caffemodel_file_name).read())\n",
    "    # C2 conv layers current require biases, but they are optional in C1\n",
    "    # Add zeros as biases is they are missing\n",
    "    add_missing_biases(caffenet_weights)\n",
    "    # We only care about getting parameters, so remove layers w/o parameters\n",
    "    remove_layers_without_parameters(caffenet, caffenet_weights)\n",
    "    # BatchNorm is not implemented in the translator *and* we need to fold Scale\n",
    "    # layers into the new C2 SpatialBN op, hence we remove the batch norm layers\n",
    "    # and apply custom translations code\n",
    "    bn_weights = remove_spatial_bn_layers(caffenet, caffenet_weights)\n",
    "    # Set num, channel, height and width for blobs that use shape.dim instead\n",
    "    normalize_shape(caffenet_weights)\n",
    "    # Translate the rest of the model\n",
    "    net, pretrained_weights = caffe_translator.TranslateModel(\n",
    "        caffenet, caffenet_weights\n",
    "    )\n",
    "    pretrained_weights.protos.extend(bn_weights)\n",
    "    return net, pretrained_weights\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = parse_args()\n",
    "#     assert os.path.exists(args.prototxt_file_name), \\\n",
    "#         'Prototxt file does not exist'\n",
    "#     assert os.path.exists(args.caffemodel_file_name), \\\n",
    "#         'Weights file does not exist'\n",
    "#     net, weights = load_and_convert_caffe_model(\n",
    "#         args.prototxt_file_name, args.caffemodel_file_name\n",
    "#     )\n",
    "#     pickle_weights(args.out_file_name, weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prototxt=\"/home/all/models/VGG_ILSVRC_16_layers_deploy.prototxt\"\n",
    "caffemodel=\"/home/all/models/VGG_ILSVRC_16_layers.caffemodel\"\n",
    "output=\"output/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "> \u001b[0;32m<ipython-input-1-d66c164158ed>\u001b[0m(195)\u001b[0;36mload_and_convert_caffe_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    194 \u001b[0;31m    \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 195 \u001b[0;31m    \u001b[0mcaffenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaffe_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    196 \u001b[0;31m    \u001b[0mcaffenet_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaffe_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-d66c164158ed>\u001b[0m(196)\u001b[0;36mload_and_convert_caffe_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    195 \u001b[0;31m    \u001b[0mcaffenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaffe_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 196 \u001b[0;31m    \u001b[0mcaffenet_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaffe_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    197 \u001b[0;31m    \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototxt_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaffenet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-d66c164158ed>\u001b[0m(197)\u001b[0;36mload_and_convert_caffe_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    196 \u001b[0;31m    \u001b[0mcaffenet_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaffe_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 197 \u001b[0;31m    \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototxt_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaffenet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    198 \u001b[0;31m    \u001b[0mcaffenet_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaffemodel_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-d66c164158ed>\u001b[0m(198)\u001b[0;36mload_and_convert_caffe_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    197 \u001b[0;31m    \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototxt_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaffenet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 198 \u001b[0;31m    \u001b[0mcaffenet_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaffemodel_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    199 \u001b[0;31m    \u001b[0;31m# C2 conv layers current require biases, but they are optional in C1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-d66c164158ed>\u001b[0m(201)\u001b[0;36mload_and_convert_caffe_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    200 \u001b[0;31m    \u001b[0;31m# Add zeros as biases is they are missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 201 \u001b[0;31m    \u001b[0madd_missing_biases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaffenet_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    202 \u001b[0;31m    \u001b[0;31m# We only care about getting parameters, so remove layers w/o parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> caffenet.layer\n",
      "[]\n",
      "ipdb> caffenet.layers\n",
      "[bottom: \"data\"\n",
      "top: \"conv1_1\"\n",
      "name: \"conv1_1\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 64\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv1_1\"\n",
      "top: \"conv1_1\"\n",
      "name: \"relu1_1\"\n",
      "type: RELU\n",
      ", bottom: \"conv1_1\"\n",
      "top: \"conv1_2\"\n",
      "name: \"conv1_2\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 64\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv1_2\"\n",
      "top: \"conv1_2\"\n",
      "name: \"relu1_2\"\n",
      "type: RELU\n",
      ", bottom: \"conv1_2\"\n",
      "top: \"pool1\"\n",
      "name: \"pool1\"\n",
      "type: POOLING\n",
      "pooling_param {\n",
      "  pool: MAX\n",
      "  kernel_size: 2\n",
      "  stride: 2\n",
      "}\n",
      ", bottom: \"pool1\"\n",
      "top: \"conv2_1\"\n",
      "name: \"conv2_1\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 128\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv2_1\"\n",
      "top: \"conv2_1\"\n",
      "name: \"relu2_1\"\n",
      "type: RELU\n",
      ", bottom: \"conv2_1\"\n",
      "top: \"conv2_2\"\n",
      "name: \"conv2_2\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 128\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv2_2\"\n",
      "top: \"conv2_2\"\n",
      "name: \"relu2_2\"\n",
      "type: RELU\n",
      ", bottom: \"conv2_2\"\n",
      "top: \"pool2\"\n",
      "name: \"pool2\"\n",
      "type: POOLING\n",
      "pooling_param {\n",
      "  pool: MAX\n",
      "  kernel_size: 2\n",
      "  stride: 2\n",
      "}\n",
      ", bottom: \"pool2\"\n",
      "top: \"conv3_1\"\n",
      "name: \"conv3_1\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 256\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv3_1\"\n",
      "top: \"conv3_1\"\n",
      "name: \"relu3_1\"\n",
      "type: RELU\n",
      ", bottom: \"conv3_1\"\n",
      "top: \"conv3_2\"\n",
      "name: \"conv3_2\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 256\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv3_2\"\n",
      "top: \"conv3_2\"\n",
      "name: \"relu3_2\"\n",
      "type: RELU\n",
      ", bottom: \"conv3_2\"\n",
      "top: \"conv3_3\"\n",
      "name: \"conv3_3\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 256\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv3_3\"\n",
      "top: \"conv3_3\"\n",
      "name: \"relu3_3\"\n",
      "type: RELU\n",
      ", bottom: \"conv3_3\"\n",
      "top: \"pool3\"\n",
      "name: \"pool3\"\n",
      "type: POOLING\n",
      "pooling_param {\n",
      "  pool: MAX\n",
      "  kernel_size: 2\n",
      "  stride: 2\n",
      "}\n",
      ", bottom: \"pool3\"\n",
      "top: \"conv4_1\"\n",
      "name: \"conv4_1\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 512\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv4_1\"\n",
      "top: \"conv4_1\"\n",
      "name: \"relu4_1\"\n",
      "type: RELU\n",
      ", bottom: \"conv4_1\"\n",
      "top: \"conv4_2\"\n",
      "name: \"conv4_2\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 512\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv4_2\"\n",
      "top: \"conv4_2\"\n",
      "name: \"relu4_2\"\n",
      "type: RELU\n",
      ", bottom: \"conv4_2\"\n",
      "top: \"conv4_3\"\n",
      "name: \"conv4_3\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 512\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv4_3\"\n",
      "top: \"conv4_3\"\n",
      "name: \"relu4_3\"\n",
      "type: RELU\n",
      ", bottom: \"conv4_3\"\n",
      "top: \"pool4\"\n",
      "name: \"pool4\"\n",
      "type: POOLING\n",
      "pooling_param {\n",
      "  pool: MAX\n",
      "  kernel_size: 2\n",
      "  stride: 2\n",
      "}\n",
      ", bottom: \"pool4\"\n",
      "top: \"conv5_1\"\n",
      "name: \"conv5_1\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 512\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv5_1\"\n",
      "top: \"conv5_1\"\n",
      "name: \"relu5_1\"\n",
      "type: RELU\n",
      ", bottom: \"conv5_1\"\n",
      "top: \"conv5_2\"\n",
      "name: \"conv5_2\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 512\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv5_2\"\n",
      "top: \"conv5_2\"\n",
      "name: \"relu5_2\"\n",
      "type: RELU\n",
      ", bottom: \"conv5_2\"\n",
      "top: \"conv5_3\"\n",
      "name: \"conv5_3\"\n",
      "type: CONVOLUTION\n",
      "convolution_param {\n",
      "  num_output: 512\n",
      "  pad: 1\n",
      "  kernel_size: 3\n",
      "}\n",
      ", bottom: \"conv5_3\"\n",
      "top: \"conv5_3\"\n",
      "name: \"relu5_3\"\n",
      "type: RELU\n",
      ", bottom: \"conv5_3\"\n",
      "top: \"pool5\"\n",
      "name: \"pool5\"\n",
      "type: POOLING\n",
      "pooling_param {\n",
      "  pool: MAX\n",
      "  kernel_size: 2\n",
      "  stride: 2\n",
      "}\n",
      ", bottom: \"pool5\"\n",
      "top: \"fc6\"\n",
      "name: \"fc6\"\n",
      "type: INNER_PRODUCT\n",
      "inner_product_param {\n",
      "  num_output: 4096\n",
      "}\n",
      ", bottom: \"fc6\"\n",
      "top: \"fc6\"\n",
      "name: \"relu6\"\n",
      "type: RELU\n",
      ", bottom: \"fc6\"\n",
      "top: \"fc6\"\n",
      "name: \"drop6\"\n",
      "type: DROPOUT\n",
      "dropout_param {\n",
      "  dropout_ratio: 0.5\n",
      "}\n",
      ", bottom: \"fc6\"\n",
      "top: \"fc7\"\n",
      "name: \"fc7\"\n",
      "type: INNER_PRODUCT\n",
      "inner_product_param {\n",
      "  num_output: 4096\n",
      "}\n",
      ", bottom: \"fc7\"\n",
      "top: \"fc7\"\n",
      "name: \"relu7\"\n",
      "type: RELU\n",
      ", bottom: \"fc7\"\n",
      "top: \"fc7\"\n",
      "name: \"drop7\"\n",
      "type: DROPOUT\n",
      "dropout_param {\n",
      "  dropout_ratio: 0.5\n",
      "}\n",
      ", bottom: \"fc7\"\n",
      "top: \"fc8\"\n",
      "name: \"fc8\"\n",
      "type: INNER_PRODUCT\n",
      "inner_product_param {\n",
      "  num_output: 1000\n",
      "}\n",
      ", bottom: \"fc8\"\n",
      "top: \"prob\"\n",
      "name: \"prob\"\n",
      "type: SOFTMAX\n",
      "]\n",
      "ipdb> caffenet_weights.layer\n",
      "[]\n",
      "ipdb> caffenet_weights.layer\n",
      "[]\n",
      "ipdb> caffenet_weights.layers\n"
     ]
    }
   ],
   "source": [
    "%pdb\n",
    "import ipdb\n",
    "net, weights = load_and_convert_caffe_model(prototxt,caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caffe2]",
   "language": "python",
   "name": "conda-env-caffe2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
